{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XNrdUJhYAIs"
      },
      "source": [
        "<h1>Content</h1>\n",
        "<ol>\n",
        "<li>Data augmentation</li>\n",
        "<li>Detection</li>\n",
        "<ol>\n",
        "<li>Training\n",
        "<li>Evaluation\n",
        "</ol>\n",
        "<li>Recognition</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bBQlpBzeaQMg"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "X778LOZAYlaj"
      },
      "outputs": [],
      "source": [
        "pwd = \"..\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRTEh_u6h4QC"
      },
      "source": [
        "# Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YL7490EwdM70"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "from PIL import Image\n",
        "import shutil\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5ZaLVo6dbNz"
      },
      "source": [
        "Set the path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "WtoGoNaRelZN"
      },
      "outputs": [],
      "source": [
        "dataset=\"custom_dataset/sak_ash_ears\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "KHfpsjKOdSBR"
      },
      "outputs": [],
      "source": [
        "dataset_path = '%s/data/datasets/%s/Images'%(pwd,dataset)\n",
        "train_path = '%s/data/data_train/%s/train'%(pwd,dataset)\n",
        "valid_path = '%s/data/data_train/%s/val'%(pwd,dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f0t22Bjhafj"
      },
      "source": [
        "Set the percentage of data for validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Y3Lv8l5ghfi5"
      },
      "outputs": [],
      "source": [
        "validation_split = 0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VoYeOtghg6y"
      },
      "source": [
        "Split datasets and move to respective directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Mkwfojl4hv6p"
      },
      "outputs": [],
      "source": [
        "# Iterate through the subject subdirectories\n",
        "for subject_dir in os.listdir(dataset_path):\n",
        "    subject_path = os.path.join(dataset_path, subject_dir)\n",
        "\n",
        "    # Create the training and validation subdirectories\n",
        "    train_subject_path = os.path.join(train_path, subject_dir)\n",
        "    valid_subject_path = os.path.join(valid_path, subject_dir)\n",
        "    os.makedirs(train_subject_path, exist_ok=True)\n",
        "    os.makedirs(valid_subject_path, exist_ok=True)\n",
        "\n",
        "    # Collect the image file paths\n",
        "    image_paths = [os.path.join(subject_path, image_file) for image_file in os.listdir(subject_path)]\n",
        "    num_images = len(image_paths)\n",
        "\n",
        "    # Shuffle the image paths\n",
        "    random.shuffle(image_paths)\n",
        "\n",
        "    # Split the dataset\n",
        "    num_valid_images = int(num_images * validation_split)\n",
        "    valid_images = image_paths[:num_valid_images]\n",
        "    train_images = image_paths[num_valid_images:]\n",
        "\n",
        "    # Move the images to the respective directories\n",
        "    for image_path in train_images:\n",
        "        shutil.copy(image_path, train_subject_path)\n",
        "\n",
        "    for image_path in valid_images:\n",
        "        shutil.copy(image_path, valid_subject_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eI3aNFTkiwW8"
      },
      "source": [
        "Training data transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "mqCBm2wLiyYr"
      },
      "outputs": [],
      "source": [
        "augment_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=(-15, 15)),\n",
        "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=(5, 5))], p=0.5),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rowbwDzi1ds"
      },
      "source": [
        "Save transformed images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "RKnpBqwqi4eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "001.ALI_HD\n",
            "002.LeDuong_BL\n",
            "003.BD_Tran\n",
            "004.Binz\n",
            "005.Bui_AT\n",
            "006.Chau_GK\n",
            "007.Chau_KP\n",
            "008.Chi_D\n",
            "009.Chi_Th\n",
            "010.Chu_B\n",
            "011.Cong_To\n",
            "012.Dai_Nhan\n",
            "013.Dam_Vinh_H\n",
            "014.Dan_Ng\n",
            "015.Dan_Trg\n",
            "016.Dang_L\n",
            "017.Dao_Ba_L\n",
            "018.De_C\n",
            "019.Don_Ng\n",
            "020.Duong_D\n",
            "021.Dustin_Phuc_Ng\n",
            "022.Duy_Kh\n",
            "023.Erik\n",
            "024.Gin_Tuan_K\n",
            "025.Ha_Anh_T\n",
            "026.Hac_Hoa_K\n",
            "027.Hamtet_Tr\n",
            "028.Ho_Quang_H\n",
            "029.Ho_Trung_D\n",
            "030.Hoai_L\n",
            "031.Hoang_Rap\n",
            "032.Huy_Tr\n",
            "033.Huynh_L\n",
            "034.Huynh_Ph\n",
            "035.Isaac\n",
            "036.Jun_Ph\n",
            "037.Justatee\n",
            "038.Kenvin_Kh\n",
            "039.KhacVi\n",
            "040.Khuong_Ng\n",
            "041.Kieu_Minh_T\n",
            "042.Kim_L\n",
            "043.L_HA\n",
            "044.Lam_Canh_T\n",
            "045.Lam_Hu\n",
            "046.Lam_Tr\n",
            "047.Lam_Vinh_H\n",
            "048.Lee_Jong_S\n",
            "049.Lee_Min_H\n",
            "050.Long_Nh\n",
            "051.Lou_Ho\n",
            "052.Luong_Bang_Q\n",
            "053.Luong_Manh_H\n",
            "054.Luong_The_Th\n",
            "055.Ly_Dich_P\n",
            "056.Mai_Tai_Ph\n",
            "057.Nam_Cuog\n",
            "058.Ngo_Diec_P\n",
            "059.Ngo_Kien_H\n",
            "060.Nguyen_Khang\n",
            "061.Nguyen_Phi_Hu\n",
            "062.Nguyen_Tran_Trung_Qu\n",
            "063.Nhan_Phuc_V\n",
            "064.Noo_Phuoc_Th\n",
            "065.Ong_Cao_Th\n",
            "066.OnlyC\n",
            "067.Pham_Hong_Ph\n",
            "068.Pham_Tr\n",
            "069.Phan_A\n",
            "070.Phan_M_Quy\n",
            "071.Quang_B\n",
            "072.Quang_Ha\n",
            "073.Quang_Tr\n",
            "074.Quy_B\n",
            "075.Rym\n",
            "076.Sky_ST\n",
            "077.Soobin_HS\n",
            "078.ST\n",
            "079.Ta_Quang_T\n",
            "080.Thai_V\n",
            "081.Thanh_D\n",
            "082.Thanh_Lo\n",
            "083.ThanhTr\n",
            "084.TIM\n",
            "085.Toulive\n",
            "086.Tran Dinh Q\n",
            "087.Tran Th\n",
            "088.Trinh_Thang_B\n",
            "089.Trong_Hie\n",
            "090.Truc_nh\n",
            "091.TrungQua\n",
            "092.TruonGNam_th\n",
            "093.TruongThe_V\n",
            "094.Tuan_Hu\n",
            "095.Tung_Dg\n",
            "096.Ung_DaiV\n",
            "097.UngHoaP\n",
            "098.V_rau\n",
            "099.Amber\n",
            "100.Angela_Ba\n",
            "101.Cao_Ng\n",
            "102.Cao_Thien_Tr\n",
            "103.Cha_Mi\n",
            "104.Chau_B\n",
            "105.Dich_Le_Nhiet\n",
            "106.Do_My_L\n",
            "107.Duong_M\n",
            "108.Gil_L\n",
            "109.Go_Joon_H\n",
            "110.H_Hen_N\n",
            "111.Hang_Ng\n",
            "112.IU\n",
            "113.Khong_Tu_Qu\n",
            "114.Kieu_Tr\n",
            "115.Kim_Hye_S\n",
            "116.Kim_Ji_W\n",
            "117.Kim_Ng\n",
            "118.Kim_Nh\n",
            "119.Kim_Ph\n",
            "120.Kim_So_H\n",
            "121.Kim_Yoo_J\n",
            "122.La_Thanh_H\n",
            "123.Lan_Kh\n",
            "124.Le_Cat_Trong_L\n",
            "125.Le_Ph\n",
            "126.Le_Thanh_Th\n",
            "127.Luu-Diec_P\n",
            "128.Ly_Nha_K\n",
            "129.Mai_H\n",
            "130.Mai_Ng\n",
            "131.Mau_Thanh_Th\n",
            "132.Mid_Ng\n",
            "133.Minh_H\n",
            "134.Minh_Tr\n",
            "135.Minh_T\n",
            "136.Miu_L\n",
            "137.My_T\n",
            "138.Ngo_Thanh_V\n",
            "139.Ngoc_Qu\n",
            "140.Nguyen_Thi_Tram\n",
            "141.Pham_H\n",
            "142.Pham_My_L\n",
            "143.Pham_Thanh_H\n",
            "144.Quynh_Anh_Sh\n",
            "145.Suzy\n",
            "146.Tam_T\n",
            "147.Thu_M\n",
            "148.Thu_Tr\n",
            "149.Thuy_H\n",
            "150.Toc_T\n",
            "151.Tran_Kieu_A\n",
            "152.Trang_K\n",
            "153.Trang_Ph\n",
            "154.Trieu_Le_D\n",
            "155.Truong_Quynh_A\n",
            "156.Tu_H\n",
            "157.Uyen_L\n",
            "158.Van_h\n",
            "159.Van_Mai_H\n",
            "160.Viet_H\n",
            "161.Vo_Hoang_Y\n",
            "162.Vu_Cat_T\n",
            "163.Xa_Thi_M\n",
            "164.Yen_Nhi_H\n"
          ]
        }
      ],
      "source": [
        "for subject in os.listdir(train_path):\n",
        "    print(subject)\n",
        "    subject_dir = os.path.join(train_path, subject)\n",
        "\n",
        "    for image_name in os.listdir(subject_dir):\n",
        "        image_path = os.path.join(subject_dir, image_name)\n",
        "\n",
        "        image = Image.open(image_path)\n",
        "        transformed_image = augment_transform(image)\n",
        "        output_path = os.path.join(subject_dir, 'aug_' + image_name)\n",
        "        transformed_image.save(output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "243MRwbGYUck"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPiucnZ7cbt0"
      },
      "source": [
        "Load a pretrained YOLO model from ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "2yDfAFJlYcR9"
      },
      "outputs": [],
      "source": [
        "model = YOLO(pwd+\"/Models/yolov8n.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnrvcDgIagJm"
      },
      "source": [
        "Train model on custom datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        },
        "id": "uj4ttyLZaf3p",
        "outputId": "7192e7a3-a981-4e90-fbbf-a802d3c8bdbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.0.232 🚀 Python-3.10.4 torch-2.1.2+cpu CPU (unknown)\n",
            "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=../Models/yolov8n.pt, data=../data/data_train/EarVN1/data.yaml, epochs=100, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train5, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train5\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Dataset '../data/data_train/EarVN1/data.yaml' error  \nDataset '../data/data_train/EarVN1/data.yaml' images not found , missing path 'D:\\Projects\\HUST\\Biometric Projects\\biometrics-ear-detection-and-recognition\\detection\\testing\\data\\data_train\\EarVN1\\valid'\nNote dataset download directory is 'D:\\Projects\\HUST\\Biometric Projects\\biometrics-ear-detection-and-recognition\\detection\\testing\\datasets'. You can update this in 'C:\\Users\\giakh\\AppData\\Roaming\\Ultralytics\\settings.yaml'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\giakh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\engine\\trainer.py:116\u001b[0m, in \u001b[0;36mBaseTrainer.__init__\u001b[1;34m(self, cfg, overrides, _callbacks)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39myaml\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39myml\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mtask \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mdetect\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msegment\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpose\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 116\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m check_det_dataset(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mdata)\n\u001b[0;32m    117\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39myaml_file\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata:\n",
            "File \u001b[1;32mc:\\Users\\giakh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\data\\utils.py:312\u001b[0m, in \u001b[0;36mcheck_det_dataset\u001b[1;34m(dataset, autodownload)\u001b[0m\n\u001b[0;32m    311\u001b[0m     m \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mNote dataset download directory is \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mDATASETS_DIR\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. You can update this in \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mSETTINGS_YAML\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 312\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(m)\n\u001b[0;32m    313\u001b[0m t \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: \nDataset '../data/data_train/EarVN1/data.yaml' images not found ⚠️, missing path 'D:\\Projects\\HUST\\Biometric Projects\\biometrics-ear-detection-and-recognition\\detection\\testing\\data\\data_train\\EarVN1\\valid'\nNote dataset download directory is 'D:\\Projects\\HUST\\Biometric Projects\\biometrics-ear-detection-and-recognition\\detection\\testing\\datasets'. You can update this in 'C:\\Users\\giakh\\AppData\\Roaming\\Ultralytics\\settings.yaml'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\giakh\\Documents\\GitHub\\PROJECT_EAR_DETECTION\\src\\detection.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/giakh/Documents/GitHub/PROJECT_EAR_DETECTION/src/detection.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mtrain(data\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m/data/data_train/EarVN1/data.yaml\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m%\u001b[39;49mpwd, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)    \u001b[39m# running time = 10 minutes\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\giakh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\engine\\model.py:351\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mresume\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    349\u001b[0m     args[\u001b[39m'\u001b[39m\u001b[39mresume\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mckpt_path\n\u001b[1;32m--> 351\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m (trainer \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_smart_load(\u001b[39m'\u001b[39;49m\u001b[39mtrainer\u001b[39;49m\u001b[39m'\u001b[39;49m))(overrides\u001b[39m=\u001b[39;49margs, _callbacks\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcallbacks)\n\u001b[0;32m    352\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m args\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mresume\u001b[39m\u001b[39m'\u001b[39m):  \u001b[39m# manually set model only if not resuming\u001b[39;00m\n\u001b[0;32m    353\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mget_model(weights\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mckpt \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, cfg\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39myaml)\n",
            "File \u001b[1;32mc:\\Users\\giakh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\engine\\trainer.py:120\u001b[0m, in \u001b[0;36mBaseTrainer.__init__\u001b[1;34m(self, cfg, overrides, _callbacks)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[\u001b[39m'\u001b[39m\u001b[39myaml_file\u001b[39m\u001b[39m'\u001b[39m]  \u001b[39m# for validating 'yolo train data=url.zip' usage\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 120\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(emojis(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDataset \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mclean_url(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdata)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m error ❌ \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainset, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtestset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_dataset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata)\n\u001b[0;32m    123\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mema \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Dataset '../data/data_train/EarVN1/data.yaml' error  \nDataset '../data/data_train/EarVN1/data.yaml' images not found , missing path 'D:\\Projects\\HUST\\Biometric Projects\\biometrics-ear-detection-and-recognition\\detection\\testing\\data\\data_train\\EarVN1\\valid'\nNote dataset download directory is 'D:\\Projects\\HUST\\Biometric Projects\\biometrics-ear-detection-and-recognition\\detection\\testing\\datasets'. You can update this in 'C:\\Users\\giakh\\AppData\\Roaming\\Ultralytics\\settings.yaml'"
          ]
        }
      ],
      "source": [
        "model.train(data=\"%s/data/data_train/EarVN1/data.yaml\"%pwd, epochs=100)    # running time = 10 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stOC_YsMf2np"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QToSnht9gGpc"
      },
      "source": [
        "### Evaluate on validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnoCF0U-gGAq"
      },
      "outputs": [],
      "source": [
        "metrics = model.val()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0_2WBbAgUoa"
      },
      "source": [
        "<h3>Evaluations on custom dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evalutate the trained model on the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_ = model.val(split='train', save_json=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evalutate the trained model on the validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_ = model.val(split='val', save_json=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Realtime Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "from ultralytics import YOLO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the YOLOv8 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = YOLO('ear_model_5_subjects.pt')\n",
        "# model = YOLO('ear_model_2_subjects.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a video capture object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vid = cv2.VideoCapture(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Detection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "while(True):\n",
        "\n",
        "    fps = vid.get(cv2.CAP_PROP_FPS)\n",
        "    print('fps:', fps)\n",
        "    # print(\"Frames per second using video.get(cv2.CAP_PROP_FPS) : {0}\".format(fps))\n",
        "\n",
        "    # Capture the video frame by frame\n",
        "    ret, frame = vid.read()\n",
        "\n",
        "    # Run YOLOv8 inference on the frame\n",
        "    results = model(frame)\n",
        "\n",
        "    # Visualize the results on the frame\n",
        "    annotated_frame = results[0].plot()\n",
        "\n",
        "    # Display the annotated frame\n",
        "    cv2.imshow(\"YOLOv8 Inference\", annotated_frame)\n",
        "\n",
        "    # # Display the resulting frame\n",
        "    # cv2.imshow('frame', frame)\n",
        "\n",
        "    # the 'q' button is set as the quitting button you may use any desired button of your choice\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# After the loop release the cap object\n",
        "vid.release()\n",
        "# Destroy all the windows\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchsummary import summary\n",
        "import time\n",
        "from datetime import timedelta\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "writer = SummaryWriter(log_dir='./runs')      \n",
        "training_dir = './test_yolo_performance/dataset/train'\n",
        "validation_dir = './test_yolo_performance/dataset/val'\n",
        "# https://www.sciencedirect.com/science/article/pii/S2352340919309850"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# input_dim = (32, 64)\n",
        "# input_dim = (64, 128)\n",
        "input_dim = (128, 256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ImageNet stats\n",
        "mean = (0.485, 0.456, 0.406)\n",
        "std = (0.229, 0.224, 0.225)\n",
        "# mean = (0.5, 0.5, 0.5)\n",
        "# std = (0.5, 0.5, 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(size=input_dim),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_dataset = torchvision.datasets.ImageFolder(root=training_dir, transform=transform)\n",
        "validation_dataset = torchvision.datasets.ImageFolder(root=validation_dir, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_batch_size = 32\n",
        "val_batch_size = 256\n",
        "train_dataloader = DataLoader(training_dataset, batch_size=train_batch_size, shuffle=True)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size=val_batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# use resnext\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnext50_32x4d', pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = model.to(device)\n",
        "summary(model, (3, input_dim[0], input_dim[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "epochs = 10\n",
        "learning_rate = 1e-3\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_function, optimizer, epoch):\n",
        "    model.train()      # set the model in training mode\n",
        "    avg_train_loss, correct = 0, 0\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        predictions = model(X)      # forward propagation\n",
        "        loss = loss_function(predictions, y)        # loss\n",
        "        avg_train_loss += loss.item()\n",
        "        optimizer.zero_grad()   # zero the parameter gradients\n",
        "        loss.backward()         # backpropagation\n",
        "        optimizer.step()        \n",
        "        _, predicted = torch.max(predictions.data, 1)  # the class with the highest energy is what we choose as prediction\n",
        "        correct += (predicted == y).sum().item()\n",
        "        if batch % 20 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    avg_train_loss /= len(dataloader)\n",
        "    train_accuracy = 100*correct/len(dataloader.dataset)\n",
        "    statistics('training', train_accuracy, avg_train_loss, epoch)\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_validation(dataloader, model, loss_function, epoch):\n",
        "    model.eval()        # set to evaluation model\n",
        "    avg_validation_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            predictions = model(images)\n",
        "            avg_validation_loss += loss_function(predictions, labels).item()       # loss\n",
        "            _, predicted = torch.max(predictions.data, 1)   # the class with the highest energy is what we choose as prediction\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    avg_validation_loss /= len(dataloader)\n",
        "    validation_accuracy = 100*correct/len(dataloader.dataset)\n",
        "    statistics('validation', validation_accuracy, avg_validation_loss, epoch)\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def statistics(dataset, accuracy, loss, epoch):\n",
        "    writer.add_scalar('Loss/' + dataset, loss, epoch)\n",
        "    writer.add_scalar('Accuracy/' + dataset, accuracy, epoch)\n",
        "    print(\"{},\\tLoss: {:.3f}\\t| Accuracy: {:.3f}\".format(dataset.title(), loss, accuracy))\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def optimize(epochs, train_dataloader, validation_dataloader, model, loss_function, optimizer):\n",
        "    start_time = time.time()\n",
        "    for i in range(epochs):\n",
        "        print(f\"\\nEpoch {i+1}\\n----------------------------------------------\")\n",
        "        train(train_dataloader, model, loss_function, optimizer, i)\n",
        "        evaluate_validation(validation_dataloader, model, loss_function, i)\n",
        "    end_time = time.time()\n",
        "    time_dif = end_time - start_time\n",
        "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimize(epochs, train_dataloader, validation_dataloader, model, loss_function, optimizer) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Finished Training')\n",
        "# training time, 3hrs 30 mins\n",
        "torch.save(model.state_dict(), \"ear_classifier.pth\")\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img = Image.open(\"./test_input_ear.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "path = \"ear_classifier.pth\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnext50_32x4d', pretrained=False)\n",
        "model = model.to(device)\n",
        "model.load_state_dict(torch.load(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ImageNet stats\n",
        "mean = (0.485, 0.456, 0.406)\n",
        "std = (0.229, 0.224, 0.225)\n",
        "# mean = (0.5, 0.5, 0.5)\n",
        "# std = (0.5, 0.5, 0.5)\n",
        "input_dim = (128, 256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(size=input_dim),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_tensor = transform(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_tensor = input_tensor.unsqueeze(0)  # Add batch dimension if needed\n",
        "input_tensor = input_tensor.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_tensor.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model(input_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interpret the output\n",
        "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "predicted_class = torch.argmax(probabilities)\n",
        "print(predicted_class+1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorboard import program\n",
        "import webbrowser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_dir = './runs/'\n",
        "tb = program.TensorBoard()\n",
        "tb.configure(argv=[None, '--logdir', log_dir])\n",
        "url = tb.launch()\n",
        "print(f\"Tensorflow listening on {url}\")\n",
        "webbrowser.open_new('http://localhost:6006/')\n",
        " \n",
        "# Kill process\n",
        "# Windows\n",
        "# netstat -ano | findstr :6006\n",
        "# taskkill /F /PID {PID}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
